{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTh-YhCJ1r5l"
   },
   "source": [
    "# Fall 2020: DS-GA 1011 NLP with Representation Learning\n",
    "## Lab 5: 02-Oct-2020, Friday\n",
    "## Distributional word representations\n",
    "In this lab, we want to explore creating vector representations of words (__distributed representations__) from co-occurence patterns in text.\n",
    "\n",
    "Note that the term __embedding__, __word vector__, or __word embedding__ can also be used for distributed representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysngTKvO1r59"
   },
   "source": [
    "---\n",
    "### Distributional matrices\n",
    "Here's a basic recipe for building a word $\\times$ word matrix:\n",
    "    \n",
    "0. Define a notion of co-occurrence context. This could be an entire document, a paragraph, a sentence, a clause, an NP â€” whatever domain seems likely to capture the associations you care about.\n",
    "0. Scan through your corpus building a dictionary $d$ mapping word-pairs to counts. Every time a pair of words $w$ and $w'$ occurs in the same context (as you defined it in 1),  increment $d[(w, w')]$ by $1$.\n",
    "0. Using the count dictionary $d$ that you collected in 2, establish your full vocabulary $V$, an ordered list of words types. For large collections of documents, $|V|$ will typically be huge. You will probably want to winnow the vocabulary at this point. You might do this by filtering to a specific subset, or just imposing a minimum count threshold. You might impose a minimum count threshold even if $|V|$ is small &mdash; for words with very low counts, you simply don't have enough evidence to say anything interesting.\n",
    "0. Now build a matrix $M$ of dimension $|V| \\times |V|$. Both the rows and the columns of $M$ represent words. Each cell $M[i, j]$ is filled with the count $d[(w_i, w_j)]$.\n",
    "\n",
    "For different designs, the procedure differs slightly. For example, if you are building a word $\\times$ document matrix, then the rows of $M$ represent words and the columns of $M$ represent documents. The scan in step 2 then just keeps track of (_word_, _document_) pairs &mdash; compiling the number of times that _word_ appears in _document_. Such matrices are often used in information retrieval, because the columns are multi-set representations of documents. They are much sparser than the the word $\\times$ word matrices we will work with here. (In my experience, they yield lower-quality lexicons, but others have reported good results with them.)\n",
    "\n",
    "Let's practice building a co-occurrence matrix. We'll use data from the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/). Download the main zip and extract the file `datasetSentences.txt`.\n",
    "\n",
    "We want to build a co-occurrence matrix where words are considered co-oocurring if they are adjacent in a sentence. Let's implement this function.\n",
    "\n",
    "Some practical notes:\n",
    "- for the project, it's okay to use outside libraries, but it's good practice to implement things from scratch\n",
    "- try to stick to built-in Python data structures as much as possible, as using library data structures can add bloat and slow things down. Building the co-occurrence matrix should generally be pretty fast as it only requires one pass over the data, except instantiating the full matrix may be somewhat slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf.\n",
    "> [`time`](https://docs.python.org/3/library/time.html) module provides various time-related functions. `time()` function returns the time in seconds since the 'epoch' (platform dependent start date) as a floating point number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6iNQbRFm1r5_"
   },
   "outputs": [],
   "source": [
    "def load_sst(data_file):\n",
    "    with open(data_file, 'r') as data_fh:\n",
    "        data_fh.readline() # skip the header\n",
    "        data = [r.split('\\t')[1] for r in data_fh.readlines()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AhMm1Nx61r6C"
   },
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(data, max_vocab_size=20000):\n",
    "    \"\"\" \n",
    "    args:\n",
    "        - data: iterable where each item is a string sentence\n",
    "        - max_vocab_size: maximum vocabulary size\n",
    "        \n",
    "    returns:\n",
    "        - coocur_mat: co-occurrence matrix as a numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_token_frequencies():\n",
    "        tok2freq = defaultdict(int)\n",
    "        coocur_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        for datum in data:\n",
    "            tokens = datum.strip().split() # we'll use whitespace to tokenize\n",
    "            for i, tok in enumerate(tokens):\n",
    "                tok2freq[tok] += 1\n",
    "                coocur_counts[tok][tok] += 1\n",
    "                if i < len(tokens) - 1:\n",
    "                    coocur_counts[tok][tokens[i+1]] += 1\n",
    "                    coocur_counts[tokens[i+1]][tok] += 1\n",
    "        return tok2freq, coocur_counts\n",
    "    \n",
    "    def prune_vocabulary(tok2freq, max_vocab_size):\n",
    "        \"\"\" Prune vocab by taking max_vocab_size most frequent words \"\"\"\n",
    "        tok_and_freqs = [(k, v) for k, v in tok2freq.items()]\n",
    "        tok_and_freqs.sort(key = lambda x: x[1], reverse=True) # sorts in-place\n",
    "        tok2idx = {tok: idx for idx, (tok, _) in enumerate(tok_and_freqs[:max_vocab_size])}\n",
    "        idx2tok = {idx: tok for tok, idx in tok2idx.items()}\n",
    "        return tok2idx, idx2tok\n",
    "    \n",
    "    def _build_coocurrence_mat(idx2tok, coocur_counts):\n",
    "        #mat = [[coocur_counts[idx2tok[i]][idx2tok[j]] for j in range(len(idx2tok))] for i in range(len(idx2tok))]\n",
    "        vocab_size = len(idx2tok)\n",
    "        mat = [[0 for _ in range(vocab_size)] for _ in range(vocab_size)]\n",
    "        for i in range(vocab_size - 1):\n",
    "            for j in range(i+1, vocab_size):\n",
    "                if coocur_counts[idx2tok[i]][idx2tok[j]]:\n",
    "                    mat[i][j] = coocur_counts[idx2tok[i]][idx2tok[j]]\n",
    "                    mat[j][i] = coocur_counts[idx2tok[i]][idx2tok[j]]\n",
    "        return np.array(mat)\n",
    "        \n",
    "    print(\"Counting words...\")\n",
    "    start_time = time.time()\n",
    "    tok2freq, coocur_counts = get_token_frequencies()\n",
    "    print(\"\\tFinished counting words in %.5f\" % (time.time() - start_time))\n",
    "\n",
    "    print(\"Pruning vocabulary...\")\n",
    "    tok2idx, idx2tok = prune_vocabulary(tok2freq, max_vocab_size)\n",
    "    start_time = time.time()\n",
    "    print(\"\\tFinished pruning vocabulary in %.5f\" % (time.time() - start_time))\n",
    "    \n",
    "    print(\"Building co-occurrence matrix...\")\n",
    "    start_time = time.time()\n",
    "    coocur_mat = _build_coocurrence_mat(idx2tok, coocur_counts)\n",
    "    print(\"\\tFinished building co-occurrence matrix in %.5f\" % (time.time() - start_time))\n",
    "    return coocur_mat, tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PXyU6SYb1r6F",
    "outputId": "dc37ce6c-4fee-4871-b71a-454b572ea453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "\tFinished counting words in 0.28962\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 24.65094\n"
     ]
    }
   ],
   "source": [
    "data_file = 'stanfordSentimentTreebank/datasetSentences.txt'\n",
    "\n",
    "data = load_sst(data_file)\n",
    "mat, tok2idx, idx2tok = build_cooccurrence_matrix(data, max_vocab_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_O9tf0u1r6N"
   },
   "source": [
    "We now have distributed, distributional word representations! Each row of this co-occurrence matrix can be seen as a word vector, nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDVmtd8_1r6N"
   },
   "source": [
    "---\n",
    "### Vector comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9pKrb4k1r6O"
   },
   "source": [
    "A key operation on word vectors is being able to compare them. For the most part, we are interested in measuring the _distance_ between vectors. We surmise that semantically related words should be close together and semantically unrelated words should be far apart in the vector spaces we build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_FgR6j41r6O"
   },
   "source": [
    "#### Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SG6XWrSA1r6P"
   },
   "source": [
    "The most basic and intuitive distance measure between vectors is __euclidean distance__. The euclidean distance between two vectors $u$ and $v$ of dimension $n$ is \n",
    "\n",
    "$$\\sqrt{\\sum_{i=1}^{n} |u_{i}-v_{i}|^2}$$ \n",
    "\n",
    "In two-dimensions, this corresponds to the length of the most direct line between the two points.\n",
    "\n",
    "As part of the exercise, implement this without using any other packages (including the one shared for reference) beyond what has been imported already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiHURlDn1r6P"
   },
   "outputs": [],
   "source": [
    "def euclidean(u, v):    \n",
    "    \"\"\"Eculidean distance between 1d np.arrays `u` and `v`, which must \n",
    "    have the same dimensionality. Returns a float.\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YR5-mLOT1r6T"
   },
   "source": [
    "Here's a tiny vector space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fFQwZyAJ1r6U",
    "outputId": "483111bf-7e4a-491a-ce3e-614a9b122dd5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ABC = np.array([\n",
    "    [ 1.0,  1.0],  # A\n",
    "    [9.0, 8.0],  # B\n",
    "    [10.0, 5.0]]) # C\n",
    "\n",
    "def plot_ABC(m):\n",
    "    plt.plot(m[:,0], m[:,1], marker='', linestyle='')\n",
    "    plt.xlim([0,np.max(m)*1.2])\n",
    "    plt.ylim([0,np.max(m)*1.2])\n",
    "    for i, x in enumerate(['A','B','C']):\n",
    "        plt.annotate(x, m[i,:])\n",
    "\n",
    "plot_ABC(ABC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AG01CTiI1r6X"
   },
   "source": [
    "The euclidean distances align well with the raw visual distance in the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9lnuK1XK1r6Z",
    "outputId": "96459305-0ef3-4cdb-ec38-3990a1101122"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.63014581273465"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(ABC[0], ABC[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LgWk2LbU1r6e",
    "outputId": "f79d8f7d-8d26-4337-c8c1-d1673a1d90a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1622776601683795"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(ABC[1], ABC[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWNEoyCv1r6h"
   },
   "source": [
    "#### Length normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cThOA-SO1r6h"
   },
   "source": [
    "Suppose we believed that vector B should be closer to vector A than to vector C. The Euclidean distances do not reflect that relationship.\n",
    "\n",
    "One strategy to impose this relationship is to normalize the vectors. We might normalize each vector by its __length__, which is defined for a vector $u$ of dimension $n$ as \n",
    "\n",
    "$$\\|u\\| = \\sqrt{\\sum_{i=1}^{n} u_{i}^{2}}$$ \n",
    "\n",
    "Please fill in `vector_length` below without using any new modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DemhWptY1r6i"
   },
   "outputs": [],
   "source": [
    "def vector_length(u):\n",
    "    \"\"\"Length (L2) of the 1d np.array `u`. Returns a new np.array with the \n",
    "    same dimensions as `u`.\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "793fI8LZ1r6l"
   },
   "source": [
    "Let's use our function to write a length-normalizing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "j0rKP3k11r6m"
   },
   "outputs": [],
   "source": [
    "def length_norm(u):\n",
    "    \"\"\"L2 norm of the 1d np.array `u`. Returns a float.\"\"\"\n",
    "    return u / vector_length(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7mzD6pj1r6t"
   },
   "source": [
    "Normalization changes the affinity or distance between points A, B, and C dramatically. Is normalization the right thing to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eDhxzpwE1r6u",
    "outputId": "f8a3a804-b387-400b-c345-459af27b4896"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_ABC(np.array([length_norm(row) for row in ABC]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikJ8E7dI1r6y"
   },
   "source": [
    "Here, the connection between A and B is more apparent, as is the opposition between B and C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehuzS5xX1r6y"
   },
   "source": [
    "#### Cosine distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNOVcQgx1r6z"
   },
   "source": [
    "Implicitly, we have replaced Euclidean distance as our distance metric with cosine distance. __Cosine distance__ takes overall length into account. The cosine distance between two vectors $u$ and $v$ of dimension $n$ is \n",
    "\n",
    "$$1 - \\left(\\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|\\cdot \\|v\\|}\\right)$$\n",
    "\n",
    "The similarity part of this (the righthand term of the subtraction) is actually measuring the _angles_ between the two vectors. The result is the same (in terms of rank order) as one gets from first normalizing both vectors using `vector_length` and then calculating their Euclidean distance.\n",
    "\n",
    "Implement this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfdooLvv1r60"
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):        \n",
    "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
    "    the same dimensionality. Returns a float.\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRqjpEqS1r63"
   },
   "source": [
    "#### Other distance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSIKjeOt1r63"
   },
   "source": [
    "It is very common to use Euclidean and cosine distance, but there are other distance measures one might consider using. For example, the particular definition of length we used is the L2-norm (because of the $^2$ exponent); we might consider using a different exponent such as $^1$, yielding the L1-norm.\n",
    "\n",
    "As practice, let's consider the Jaccard distance, which is defined as \n",
    "\n",
    "$$ J(u, v) = 1 - \\left( \\frac{\\sum_{i} min(u_i, v_i)}{\\sum_{i} max(u_i, v_i) } \\right) $$\n",
    "\n",
    "Jaccard distance measures how dissimilar two vectors are by comparing their \"intersection\" and \"union\" (it's usually defined in context of sets). Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDFMmw731r64"
   },
   "outputs": [],
   "source": [
    "def jaccard(u, v):\n",
    "    \"\"\"Jaccard similarity between two real-valued vectors\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbywH1991r6-"
   },
   "source": [
    "A recent line of work is in learning _hyperbolic_ word embeddings, where the embeddings live in a non-Euclidean, hyperbolic space. The appeal of this approach is that the learned embeddings can respect the hierarchical relationship that some words have with each other.\n",
    "\n",
    "For more information, see [Nickel and Kiela (2017)](https://arxiv.org/abs/1705.08039) or [Chamberlain et al. (2017)](https://arxiv.org/abs/1705.10359)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-QVuUYo1r6_"
   },
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El7nMhrI1r6_"
   },
   "source": [
    "Suppose we set for ourselves the goal of associating A with B and disassociating B from C, in keeping with the semantic intuition expressed above. Then we can assess distance measures by whether they achieve this goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5W2-qgJ51r7A",
    "outputId": "9bb09956-aeee-4b0b-fa33-1e3c169a36c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      euclidean(A, B) = 10.630       euclidean(B, C) = 3.162\n",
      "         cosine(A, B) = 0.002          cosine(B, C) = 0.034\n",
      "        jaccard(A, B) = 0.882         jaccard(B, C) = 0.222\n"
     ]
    }
   ],
   "source": [
    "for m in (euclidean, cosine, jaccard):\n",
    "    fmt = {'n': m.__name__,  \n",
    "           'AB': m(ABC[0], ABC[1]), \n",
    "           'BC': m(ABC[1], ABC[2])}\n",
    "    print('%(n)15s(A, B) = %(AB)5.3f %(n)15s(B, C) = %(BC)5.3f' % fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf.\n",
    ">[`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) or [`scipy.linalg.norm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.norm.html#scipy.linalg.norm) returns vector or matrix norm\n",
    "\n",
    ">[`scipy.spatial.distance.cdist`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html) computes distance between each pair of the two collections of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCPLFuGR1r7D"
   },
   "source": [
    "---\n",
    "### Distributional neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gckvHh9R1r7D"
   },
   "source": [
    "The `neighbors` function is an investigative aide. For a given `word`, it ranks all the words in the vocabulary `rownames` according to their distance from `word`, as measured by `distfunc` in matrix `mat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OxV1fh1R1r7E"
   },
   "outputs": [],
   "source": [
    "def neighbors(tok, mat, tok2idx, idx2tok, distfunc=cosine):    \n",
    "    \"\"\"Tool for finding the nearest neighbors of `word` in `mat` according \n",
    "    to `distfunc`. The comparisons are between row vectors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tok : str\n",
    "        The anchor word. Assumed to be in `tok2idx`.\n",
    "        \n",
    "    mat : np.array\n",
    "        The vector-space model.\n",
    "        \n",
    "    tok2idx : list of str\n",
    "        The rownames of mat.\n",
    "            \n",
    "    distfunc : function mapping vector pairs to floats (default: `cosine`)\n",
    "        The measure of distance between vectors. Can also be `euclidean`, \n",
    "        `matching`, `jaccard`, as well as any other distance measure  \n",
    "        between 1d vectors.\n",
    "    \n",
    "    Returns\n",
    "    -------    \n",
    "    list of tuples\n",
    "        The list is ordered by closeness to `word`. Each member is a pair \n",
    "        (word, distance) where word is a str and distance is a float.\n",
    "    \n",
    "    \"\"\"\n",
    "    if tok not in tok2idx:\n",
    "        raise ValueError('%s is not in this VSM' % tok)\n",
    "    w = mat[tok2idx[tok]]\n",
    "    dists = [(idx2tok[i], distfunc(w, mat[i])) for i in range(len(mat))]\n",
    "    return sorted(dists, key=lambda x: x[1], reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yU9cN68s1r7H"
   },
   "source": [
    "By playing around with this function, you can start to get a sense for how the distance functions differ. Here are some example calls; you might try some new words to get a feel for what these matrices are like and how different words look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AHfP4rOe1r7H",
    "outputId": "11b45dd5-4e58-4234-d5e8-c57efd755b07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', 0.0),\n",
       " ('cipher', 4.69041575982343),\n",
       " ('illuminated', 4.69041575982343),\n",
       " ('Using', 4.795831523312719),\n",
       " ('spine', 4.795831523312719),\n",
       " ('legend', 4.898979485566356),\n",
       " ('shining', 4.898979485566356),\n",
       " ('lick', 4.898979485566356),\n",
       " ('bravura', 4.898979485566356),\n",
       " ('stellar', 4.898979485566356)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(tok='superb', mat=mat, tok2idx=tok2idx, idx2tok=idx2tok, distfunc=euclidean)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "huGPlxo01r7L",
    "outputId": "249dd219-106a-471b-a9b2-d82ae3fb5495"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-f8317b7dac6a>:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('superb', 1.1102230246251565e-16),\n",
       " ('cipher', 0.5219085562662426),\n",
       " ('fellow', 0.5271337562565397),\n",
       " ('illuminated', 0.527544408738466),\n",
       " ('terrific', 0.5281792278953805),\n",
       " ('solid', 0.538018447402505),\n",
       " ('wonderful', 0.5496408554571595),\n",
       " ('fault', 0.5552504100033393),\n",
       " ('good', 0.560877926872458),\n",
       " ('Using', 0.5635642195280153)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(tok='superb', mat=mat, tok2idx=tok2idx, idx2tok=idx2tok, distfunc=cosine)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68VYjf-i1r7R"
   },
   "source": [
    "These rankings are okay, with `cosine` less likely to associate words that happen to have similar frequency, but somewhat noisy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKZOrf0-1r7R"
   },
   "source": [
    "---\n",
    "### Matrix reweighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePmi0bJc1r7S"
   },
   "source": [
    "*Reweighting* aims to amplify the important, trustworthy, and unusual, while deemphasizing the mundane and the quirky. The intuition behind moving away from raw counts is that just using frequency is too fuzzy a concept for our goal of encoding semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzTE00t51r7S"
   },
   "source": [
    "*Normalization* (row-wise or column-wise) is perhaps the simplest form of reweighting. With [length_norm](#Length-normalization), we normalize using `vector_length`. We can also normalize each row by the sum of its values, which turns each row into a probability distribution over the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "We2waLUl1r7T"
   },
   "outputs": [],
   "source": [
    "def prob_norm(u):\n",
    "    \"\"\"Normalize 1d np.array `u` into a probability distribution. Assumes \n",
    "    that all the members of `u` are positive. Returns a 1d np.array of \n",
    "    the same dimensionality as `u`.\"\"\"\n",
    "    return u / np.sum(u)\n",
    "\n",
    "def rowwise_norm_mat(mat):\n",
    "    return np.array([prob_norm(u) for u in mat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TLfKgioK1r7W",
    "outputId": "e41d8848-a4d3-4a70-ddb0-5b4d33f79e59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-c7b72c0a97d4>:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return u / np.sum(u)\n"
     ]
    }
   ],
   "source": [
    "norm_mat = rowwise_norm_mat(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wev6Vzex1r7Y"
   },
   "source": [
    "Key point: These normalization measures are insensitive to the _magnitude_ of the underlying counts. \n",
    "\n",
    "This is often a mistake in the messy world of large data sets; $[1,10]$ and $[1000,10000]$ are very different in ways that will be partly or totally obscured by normalization.\n",
    "\n",
    "How do we solve this? Pointwise mutual information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWZa2d1N1r7Z"
   },
   "source": [
    "---\n",
    "### Dimensionality reduction\n",
    "The above methods deliver solid results. However, they are not capable of capturing higher-order associations in the data. For example, both _gnarly_ and _wicked_ are used as slangily positive adjectives. We thus expect them to have many of the same neighbors. However, at least stereotypically, _gnarly_ is Californian and _wicked_ is Bostonian. Thus, they are unlikely \n",
    "to occur often in the same texts. Dimensionality reduction techniques are often capable of capturing their semantic similarity (and have the added advantage of shrinking the size of our data structures).\n",
    "\n",
    "The general goal of dimensionality reduction is to eliminate rows/columns that are highly correlated while bringing similar things together and pushing dissimilar things apart. __Latent Semantic Analysis__ (LSA) is a prominent method. It is an application of truncated __singular value decomposition__ (SVD). SVD is a central matrix operation; 'truncation' here means looking only at submatrices of the full decomposition. LSA seeks not only to find a reduced-sized matrix but also to capture similarities that come not just from direct co-occurrence, but also from second-order co-occurrence.\n",
    "\n",
    "Dimensionality reduction is also particularly helpful in our case because our vectors are very sparse. Intuitively, many words don't often appear right next to each other, so many entries in the word vectors are zero. By reducing our word vectors, we won't need to pass around very large vectors and remove some of the sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf.\n",
    ">[`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html) performs Singular Value Decomposition. Also, [`scipy.linalg.svd`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html#scipy.linalg.svd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JynMpzvJ1r7a"
   },
   "outputs": [],
   "source": [
    "def lsa(mat=None, k=100):\n",
    "    \"\"\"Latent Semantic Analysis using pure scipy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mat : 2d np.array\n",
    "       The matrix to operate on.\n",
    "        \n",
    "    k : int (default: 100)\n",
    "        Number of dimensions to truncate to.\n",
    "        \n",
    "    Returns\n",
    "    -------    \n",
    "    (np.array, list of str)\n",
    "        The first member is the SVD-reduced version of `mat` with \n",
    "        dimension (m x k), where m is the rowcount of mat and `k` is \n",
    "        either the user-supplied k or the column count of `mat`, whichever \n",
    "        is smaller. The second member is `rownames` (unchanged).\n",
    "\n",
    "    \"\"\"    \n",
    "    rowmat, singvals, colmat = svd(mat, full_matrices=False)\n",
    "    singvals = np.diag(singvals)\n",
    "    trunc = np.dot(rowmat[:, 0:k], singvals[0:k, 0:k])\n",
    "    return trunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFnrftSV1r7g"
   },
   "source": [
    "Here's a look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WuwepFNc1r7h"
   },
   "outputs": [],
   "source": [
    "gnmat = np.array([\n",
    "    [1,0,1,0,0,0],\n",
    "    [0,1,0,1,0,0],\n",
    "    [1,1,1,1,0,0],\n",
    "    [0,0,0,0,1,1],\n",
    "    [0,0,0,0,0,1]], dtype='float64')\n",
    "gn_idx2tok = {0:'gnarly', 1:'wicked', 2:'awesome', 3:'lame', 4:'terrible'}\n",
    "gn_tok2idx = {v: k for k, v in gn_idx2tok.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hsQGKgjO1r7m",
    "outputId": "c99fa815-aace-4975-914d-601201dbaacb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gnarly', 2.220446049250313e-16),\n",
       " ('awesome', 0.29289321881345254),\n",
       " ('wicked', 1.0),\n",
       " ('lame', 1.0),\n",
       " ('terrible', 1.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(tok='gnarly', mat=gnmat, tok2idx=gn_tok2idx, idx2tok=gn_idx2tok, distfunc=cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVBDo9Lg1r7t"
   },
   "source": [
    "We see that _gnarly_ and _wicked_ are not close to each other. (Well, it's a small space, but they are as close as _gnarly_ and _lame_.) Reweighting by PMI, PPMI, or TF-IDF is no help. LSA to the rescue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RhVUszJ81r7v"
   },
   "outputs": [],
   "source": [
    "gnmat_lsa = lsa(mat=gnmat, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yO_BJWLj1r70",
    "outputId": "909f3b3b-8302-4bea-e2fa-a727d1777760"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gnarly', 0.0),\n",
       " ('wicked', 0.0),\n",
       " ('awesome', 0.0),\n",
       " ('lame', 1.0),\n",
       " ('terrible', 1.0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(tok='gnarly', mat=gnmat_lsa, tok2idx=gn_tok2idx, idx2tok=gn_idx2tok, distfunc=cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8r5dBS01r75"
   },
   "source": [
    "---\n",
    "### Evaluation: Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUcXA-Dn1r77"
   },
   "source": [
    "We've done a good deal of work creating these word vectors, but how do we know how good they are? Inspecting the neighbors of words according to their vectors and making sure they match our intuition is a good hueristic.\n",
    "\n",
    "Expanding on this idea, we can evaluate the quality of word vectors by using them to compute similarities of pairs of words and then comparing those similarities to human judgments. Let's use this strategy to evaluate the word vectors we just learned. We'll use the [MTurk-771](http://www2.mta.ac.il/~gideon/mturk771.html) word similarity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "H_KF589D1r77"
   },
   "outputs": [],
   "source": [
    "def load_word_similarity_dataset(data_file):\n",
    "    with open(data_file, 'r') as data_fh:\n",
    "        raw_data = data_fh.readlines()\n",
    "    data = []\n",
    "    trgs = []\n",
    "    for datum in raw_data:\n",
    "        datum = datum.strip().split(',')\n",
    "        data.append((datum[0], datum[1]))\n",
    "        trgs.append(float(datum[2]))\n",
    "    return data, trgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Hkd3Hq0B1r8D"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf.\n",
    ">[`scipy.stats.spearmanr`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html) calculates Spearman correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_word_similarity(word_pairs, targets, mat, tok2idx):\n",
    "    \"\"\" \"\"\"\n",
    "    preds = []\n",
    "    trgs = []\n",
    "    n_exs = 0\n",
    "    for (word1, word2), trg in zip(word_pairs, targets):\n",
    "        if word1 in tok2idx and word2 in tok2idx:\n",
    "            pred_sim = 1 - cosine(mat[tok2idx[word1]], mat[tok2idx[word2]])\n",
    "            preds.append(pred_sim)\n",
    "            trgs.append(trg)\n",
    "            n_exs += 1\n",
    "    \n",
    "    rho, pvalue = spearmanr(trgs, preds)\n",
    "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(word_pairs)))\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "spokZEV91r8G"
   },
   "outputs": [],
   "source": [
    "test_file = 'MTURK-771.csv'\n",
    "test_data, test_trgs = load_word_similarity_dataset(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Lp5ailUy1r8T",
    "outputId": "225fe037-27f5-4c4d-a988-630ecde602a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 248 of 771 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.024826560560269804"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_word_similarity(test_data, test_trgs, norm_mat, tok2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcmY48ST1r8V"
   },
   "source": [
    "---\n",
    "### Pre-trained Word Embeddings/Models\n",
    "Our word vectors aren't very good (NLP isn't that easy...). There are some great pre-computed matrices available online too. These aren't matrices of counts, but rather more abstract values computed using methods like those under discussion here. [GloVe](https://nlp.stanford.edu/projects/glove/) is a unsupervised learning algorithm that learns vector representations of words using word-to-word cooccurence matrices from a corpus. \n",
    "\n",
    "Code below loads the GloVe vectors (you don't necessarily need to load *all* of them) and evaluates them on MTurk-771."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_file, n_vecs=20000):\n",
    "    \"\"\" \"\"\"\n",
    "    tok2vec = {}\n",
    "    with open(glove_file, 'r') as glove_fh:\n",
    "        for i, row in enumerate(glove_fh):\n",
    "            word, vec = row.split(' ', 1)\n",
    "            tok2vec[word] = np.array([float(n) for n in vec.split(' ')])\n",
    "            if i >= n_vecs:\n",
    "                break\n",
    "    return tok2vec\n",
    "\n",
    "def glove_evaluate_word_similarity(word_pairs, targets, glove_vecs):\n",
    "    \"\"\" \"\"\"\n",
    "    preds = []\n",
    "    trgs = []\n",
    "    n_exs = 0\n",
    "    for (word1, word2), trg in zip(word_pairs, targets):\n",
    "        if word1 in glove_vecs and word2 in glove_vecs:\n",
    "            pred_sim = 1 - cosine(glove_vecs[word1], glove_vecs[word2])\n",
    "            preds.append(pred_sim)\n",
    "            trgs.append(trg)\n",
    "            n_exs += 1\n",
    "    \n",
    "    rho, pvalue = spearmanr(trgs, preds)\n",
    "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(word_pairs)))\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_file = \"glove.840B.300d.txt\"\n",
    "glove_file = \"glove/glove.6B.300d.txt\"\n",
    "\n",
    "glove_vecs = load_glove(glove_file, n_vecs=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 771 of 771 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6500829276838272"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_evaluate_word_similarity(test_data, test_trgs, glove_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other popular open-source models/libraries:\n",
    "- [word2vec](https://code.google.com/archive/p/word2vec/) (Google): This tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words.\n",
    "- [fastText](https://fasttext.cc) (Facebook): FastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTxubJDgL0rh"
   },
   "source": [
    "---\n",
    "## References\n",
    "- CS224U Natural Language Understanding Spring 2016\n",
    "- DS-GA 1012 Natural Language Understanding Spring 2019"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "05-embedding.ipynb",
   "provenance": [
    {
     "file_id": "1RoEvjZEdfJNPXL-qxLo54RtGtWg5rSba",
     "timestamp": 1601405033893
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
