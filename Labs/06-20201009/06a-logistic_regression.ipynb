{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"base"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"06a-logistic_regression.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"WJHet82BHfGJ"},"source":["# Fall 2020: DS-GA 1011 NLP with Representation Learning\n","## Lab 6: 09-Oct-2020, Friday\n","## Sentiment analysis using Logistic Regression\n","\n","In this lab, we'll go through the process of processing a dataset, designing features, fitting a model on the feature data (sort of), and evaluate on a held-out test set."]},{"cell_type":"markdown","metadata":{"id":"Ab1uPbPaHfGN"},"source":["---\n","### Setup"]},{"cell_type":"markdown","metadata":{"id":"a4nnoTuXHfGP"},"source":["First, let's load the Stanford Sentiment Treebank. Download it from here: [the train/dev/test Stanford Sentiment Treebank distribution](http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip), unzip it, and put the resulting folder in the same directory as this notebook. (If you want to put it somewhere else, change `sst_home` below.)"]},{"cell_type":"code","metadata":{"id":"rdXPlLLYHfGR"},"source":["# Import required packages\n","import re\n","import os\n","import numpy as np\n","import collections"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7YUERnTbHfGc"},"source":["sst_home = 'trees'\n","\n","def load_sst_data(path):\n","    # Let's do 2-way positive/negative classification instead of 5-way\n","    EASY_LABEL_MAP = {0:0, 1:0, 2:None, 3:1, 4:1}\n","    \n","    data = []\n","    with open(path) as f:\n","        for line in f: \n","            example = {}\n","            example['label'] = EASY_LABEL_MAP[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","\n","    return data\n","\n","train = load_sst_data(sst_home + '/train.txt')\n","val = load_sst_data(sst_home + '/dev.txt')\n","test = load_sst_data(sst_home + '/test.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJCorhH1HfGi","outputId":"ae408794-7c44-48cf-99ae-5406cb7fb755"},"source":["train[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': 1,\n"," 'text': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\"}"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"gJTsxR01HfGo","outputId":"d1461427-de44-4110-cd86-45d142c605e1"},"source":["print(len(train), len(val), len(test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["6920 872 1821\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-fvBUiGxHfGs"},"source":["---\n","### Extracting features"]},{"cell_type":"markdown","metadata":{"id":"g75CLN9PHfGt"},"source":["Now that we have the data, we need to build some sort of feature representation of our data. One of the simplest things we can do is to represent each sentence as a bag of its words. "]},{"cell_type":"code","metadata":{"id":"awx_tIkUHfGu"},"source":["def tokenize(string):\n","    ''' Bare-bones tokenization '''\n","    return string.split() # simple tokenization\n","\n","def extract_feats(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","                         \n","    # Extract vocabulary\n","    word_counter = collections.Counter()\n","    for example in datasets[0]: # assume first dataset is training set\n","        word_counter.update(tokenize(example['text']))\n","    vocabulary = set(word_counter.keys())\n","\n","    features = set()\n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['features'] = collections.defaultdict(float)\n","            \n","            #Extract features (by name) for one example:\n","            word2count = collections.Counter(tokenize(example['text']))\n","            for word, count in word2count.items():\n","                if word in vocabulary:\n","                    example[\"features\"][word] = min(count, 1) # BoW binary features\n","            \n","            features.update(example['features'].keys())\n","                            \n","    # By now, we know what all the features will be, so we can\n","    # assign indices to them.\n","    feat2idx = dict(zip(features, range(len(features))))\n","    idx2feat = {v: k for k, v in feat2idx.items()}\n","    dim = len(feat2idx)\n","                \n","    # Now we create actual vectors from those indices.\n","    for dataset in datasets:\n","        for example in dataset:\n","            example['input'] = np.zeros((dim))\n","            for feature in example['features']:\n","                example['input'][feat2idx[feature]] = example['features'][feature]\n","    return idx2feat\n","    \n","idx2feat = extract_feats([train, val, test]) # adds the features as a key in each example dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uynpmz_cHfG0","outputId":"5227334f-43a1-4db2-b64a-97bb9907a0eb"},"source":["len(idx2feat)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16282"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"P4af8YcFHfG4","outputId":"8aa38aff-029b-4a90-aa61-a676fa474312"},"source":["for key in range(25):\n","    print(key, idx2feat[key])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 remarkable\n","1 emptiness\n","2 unthinkable\n","3 remarkably\n","4 omission\n","5 Unambitious\n","6 because\n","7 Wildly\n","8 Armageddon\n","9 child\n","10 dictator-madman\n","11 Blind\n","12 giants\n","13 flatter\n","14 enigma\n","15 good-bad\n","16 sparked\n","17 duty\n","18 amoral\n","19 sucking\n","20 nursery\n","21 waif\n","22 Cusack\n","23 Oliver\n","24 upon\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5X7KHCeQHfG8","outputId":"1bf1c04c-a1ac-4cb3-94d6-35ee96fc4a83"},"source":["train[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': 1,\n"," 'text': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\",\n"," 'features': defaultdict(float,\n","             {'The': 1,\n","              'Rock': 1,\n","              'is': 1,\n","              'destined': 1,\n","              'to': 1,\n","              'be': 1,\n","              'the': 1,\n","              '21st': 1,\n","              'Century': 1,\n","              \"'s\": 1,\n","              'new': 1,\n","              '``': 1,\n","              'Conan': 1,\n","              \"''\": 1,\n","              'and': 1,\n","              'that': 1,\n","              'he': 1,\n","              'going': 1,\n","              'make': 1,\n","              'a': 1,\n","              'splash': 1,\n","              'even': 1,\n","              'greater': 1,\n","              'than': 1,\n","              'Arnold': 1,\n","              'Schwarzenegger': 1,\n","              ',': 1,\n","              'Jean-Claud': 1,\n","              'Van': 1,\n","              'Damme': 1,\n","              'or': 1,\n","              'Steven': 1,\n","              'Segal': 1,\n","              '.': 1}),\n"," 'input': array([0., 0., 0., ..., 0., 0., 0.])}"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"afZ0-wsdHfHA","outputId":"df78491f-9646-4bee-8d2d-47be6a153e55"},"source":["train[0]['input'].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16282,)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"kD5TiL-pHfHF","outputId":"0925e956-8284-4f73-e2df-92e93c3a9bb8"},"source":["X_train = [x['input'] for x in train]\n","y_train = [y['label'] for y in train]\n","print(len(X_train), X_train[0].shape, len(y_train), y_train[0])\n","print(X_train[0].nonzero(), y_train[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["6920 (16282,) 6920 1\n","(array([  435,  3055,  3469,  3569,  3792,  4426,  4785,  5169,  5870,\n","        7017,  7322,  7610,  8171,  8413,  8889,  8948,  9609, 10032,\n","       10129, 10146, 10890, 11240, 11651, 11686, 11847, 11895, 11954,\n","       13041, 13299, 13733, 14488, 14539, 15090, 15671]),) 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OT4IIdfCHfHJ"},"source":["#### Pre-packaged methods"]},{"cell_type":"markdown","metadata":{"id":"hJMAQJMvHfHK"},"source":["cf.\n","> [scikit-learn](https://scikit-learn.org/stable/) Open-source machine learning library providing simple and efficient tools for  predictive data analysis using Python. Built on NumPy, SciPy, and matplotlib\n","\n","> [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer) Transformer that converts a collection of text documents to a matrix of token counts"]},{"cell_type":"code","metadata":{"id":"Cn_3iw7bHfHK","outputId":"acc99a99-1aa9-4f8e-9318-6573f1110da8"},"source":["# Install scikit-learn\n","# !conda install scikit-learn\n","!conda list scikit-learn"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# packages in environment at /opt/anaconda3:\n","#\n","# Name                    Version                   Build  Channel\n","scikit-learn              0.23.1           py38h603561c_0  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YO6GkhsWHfHO","outputId":"9115411f-e1b0-4c45-f87c-5a550e302730"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","train_docs = [doc['text'] for doc in train]\n","vectorizer = CountVectorizer(lowercase=False, binary=True)\n","vectorizer.fit(train_docs)\n","vocab = vectorizer.get_feature_names()\n","print(len(vocab), vocab[:25])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["15255 ['000', '10', '100', '101', '103', '105', '10th', '11', '110', '112', '12', '120', '127', '129', '12th', '13', '13th', '14', '140', '146', '15', '15th', '16', '163', '168']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h013tY6qHfHS","outputId":"69adec01-6dea-4b7a-bd74-bec69c6e4512"},"source":["X_train_sk = vectorizer.transform(train_docs)\n","y_train_sk = np.array([doc['label'] for doc in train])\n","print(X_train_sk.shape, y_train_sk.shape)\n","print(y_train_sk[0], type(X_train_sk), '\\n', X_train_sk[0], )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(6920, 15255) (6920,)\n","1 <class 'scipy.sparse.csr.csr_matrix'> \n","   (0, 70)\t1\n","  (0, 298)\t1\n","  (0, 711)\t1\n","  (0, 789)\t1\n","  (0, 851)\t1\n","  (0, 949)\t1\n","  (0, 1856)\t1\n","  (0, 2897)\t1\n","  (0, 3014)\t1\n","  (0, 3048)\t1\n","  (0, 3287)\t1\n","  (0, 3426)\t1\n","  (0, 3606)\t1\n","  (0, 4232)\t1\n","  (0, 4658)\t1\n","  (0, 6442)\t1\n","  (0, 7286)\t1\n","  (0, 8213)\t1\n","  (0, 8299)\t1\n","  (0, 8499)\t1\n","  (0, 9284)\t1\n","  (0, 9863)\t1\n","  (0, 10458)\t1\n","  (0, 10693)\t1\n","  (0, 13208)\t1\n","  (0, 13939)\t1\n","  (0, 13943)\t1\n","  (0, 13944)\t1\n","  (0, 14099)\t1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wyGevM87HfHV"},"source":["---\n","### Building a Model: Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"u4vTS9vVHfHW"},"source":["Let's build a classifier for this dataset. Because we haven't talked about regularization yet, we’ll use the LogisticRegression class from scikit-learn using out-of-the-box solver (non-SGD) and no regularization (penalty)."]},{"cell_type":"code","metadata":{"id":"3NOgNVlqHfHW"},"source":["from sklearn.linear_model import LogisticRegression\n","log_model = LogisticRegression(penalty='none')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zh4TtNnbHfHZ"},"source":["cf.\n","> [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression) An estimator for classification using Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"Vk7kWVlNHfHZ"},"source":["In order to learn the \"best\" parameters for our model based on the training data, we use scikit-learn’s fit method. Inside this method, the parameters are according to some loss function (see slides)."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"SQ1jauJHHfHa","outputId":"e45516cd-8657-49ef-ea59-8ec336e2a3d4"},"source":["log_model.fit(X=X_train, y=y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(penalty='none')"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"QcILJBwdHfHc"},"source":["We now have a trained sentiment analysis model! Let's predict using the same."]},{"cell_type":"code","metadata":{"id":"KFsbTLxQHfHd","outputId":"482d7225-8592-4cea-b27f-46619217722f"},"source":["y_preds = log_model.predict(X_train)\n","print('Review: ', train[0]['text'], '\\n\\nLabel: ', train[0]['label'], ' Prediction: ', y_preds[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Review:  The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal . \n","\n","Label:  1  Prediction:  1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qQYevRdoHfHh"},"source":["---\n","### Evaluating a Model and Extensions"]},{"cell_type":"markdown","metadata":{"id":"SkklP5YTHfHh"},"source":["How well does our model do? Let's define a function to see our model's accuracy on some data split and see how well we fit the training data. We'll make use of the `model.predict()` interface for generating predictions."]},{"cell_type":"code","metadata":{"id":"0oIhvcNEHfHh"},"source":["from sklearn.metrics import accuracy_score\n","\n","def evaluate(inputs, targs, model):\n","    preds = model.predict(inputs)\n","    return accuracy_score(preds, targs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BfOtfxM1HfHl"},"source":["cf.\n","> [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?highlight=accuracy%20score#sklearn.metrics.accuracy_score) Classification score"]},{"cell_type":"code","metadata":{"id":"uFDXrfK7HfHl","outputId":"94e362a3-6977-4944-ef42-7c97b02d6bc7"},"source":["X_train = [x['input'] for x in train]\n","y_train = [y['label'] for y in train]\n","train_acc = evaluate(X_train, y_train, log_model)\n","print(\"Train acc: %.3f\" % (100 * train_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train acc: 100.000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"km66HGmsHfHo"},"source":["Nice, 100% accuracy. How well do we do on held-out data?"]},{"cell_type":"code","metadata":{"id":"0oZXo_SYHfHo","outputId":"22be2cf5-0e96-4af9-a97c-97eb071a7a6d"},"source":["X_dev = [x['input'] for x in val]\n","y_dev = [y['label'] for y in val]\n","dev_acc = evaluate(X_dev, y_dev, log_model)\n","print(\"Dev acc: %.3f\" % (100 * dev_acc)) # log_model.score(X_dev, y_dev)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dev acc: 76.491\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EdCopTj6HfHr"},"source":["We see a big drop, ~25 accuracy, on held-out data, so we overfit the training data. We can go back and revise our approach (e.g. by playing around with the different parameters for the [`LogisticRegression` classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)) and re-fitting on the training data, and then see how well we do on the held-out validation data.\n","\n","By doing this, however, we'll be fitting to the validation data. At some point, we'll want to evaluate one completely new data. Which is what the test split is for. The test split should be used as sparingly as possible!"]},{"cell_type":"code","metadata":{"id":"TVYVaIZHHfHr","outputId":"eb0a20f2-7dbf-440e-aaf6-68ad1dd7cb8a"},"source":["X_test = [x['input'] for x in test]\n","y_test = [y['label'] for y in test]\n","test_acc = evaluate(X_test, y_test, log_model)\n","print(\"Test acc: %.3f\" % (100 * test_acc)) # log_model.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test acc: 78.309\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IlFMV5cQHfHw"},"source":["We've been evaluating on data drawn roughly from the same data distribution. How do our models fare if we move out-of-distribution? We will be using IMDb movie reviews as a test set later on. Download the data <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">here</a>, unzip it, and put the resulting folder in the same directory as this notebook.\n","\n","The following function reformats it in the same form as our SST data."]},{"cell_type":"code","metadata":{"id":"_o8oPGIqHfHy"},"source":["imdb_home = 'aclImdb/test/'\n","\n","def load_imdb_data(path):\n","    \n","    pos_data, neg_data = [], []\n","    all_files = []\n","    _limit = 250\n","    \n","    for dirpath, dirnames, files in os.walk(path):\n","        for name in files:\n","            all_files.append(os.path.join(dirpath, name))\n","            \n","            \n","    for file_path in all_files:\n","        if '/neg' in file_path and len(neg_data) <= _limit:\n","            example = {}\n","            with open(file_path, 'r') as myfile:\n","                example['text'] = myfile.read().replace('\\n', '')\n","            example['label'] = 0\n","            neg_data.append(example)\n","            \n","        if '/pos' in file_path and len(pos_data) <= _limit:\n","            example = {}\n","            with open(file_path, 'r') as myfile:\n","                example['text'] = myfile.read().replace('\\n', '')\n","            example['label'] = 1\n","            pos_data.append(example)\n","    data = neg_data + pos_data\n","\n","    return data\n","\n","            \n","imdb_test = load_imdb_data(imdb_home)\n","idx2feat = extract_feats([train, imdb_test]) # adds the features as a key in each example dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kI5gy_cFHfH5","outputId":"4dc9becb-2dcb-4f94-e743-72b324736100"},"source":["X_test_imdb = [x['input'] for x in imdb_test]\n","y_test_imdb = [y['label'] for y in imdb_test]\n","test_acc = evaluate(X_test_imdb, y_test_imdb, log_model)\n","print(\"IMDb Test acc: %.3f\" % (100 * test_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["IMDb Test acc: 75.299\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GI4Cdsr_HfH-","outputId":"4ad22b28-3850-4a0d-9135-f70432bd41a0"},"source":["coefficients = log_model.coef_[0]\n","print(coefficients.shape)\n","indices = np.argsort(coefficients) \n","# Most negatively weighted\n","print(\"\\nWords associated with negative sentiment\")\n","for i in indices[:10]:\n","  print(idx2feat[i], coefficients[i])\n","print()\n","# Most positively weighted\n","print(\"Words associated with positive sentiment\")\n","for i in indices[-10:]: \n","  print(idx2feat[i], coefficients[i])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(16282,)\n","\n","Words associated with negative sentiment\n","stupid -139.38785286072397\n","mess -136.4392950346883\n","depressing -117.72992560143074\n","suffers -106.96891312120053\n","flat -106.71797240294553\n","worst -105.64415172590596\n","none -98.06556852228475\n","failure -96.80920673401636\n","TV -96.58653776541415\n","lacking -95.90210240367811\n","\n","Words associated with positive sentiment\n","rare 92.33004804385504\n","half-bad 94.6275338547006\n","charming 99.73731230769108\n","refreshing 100.57417260805157\n","hilarious 102.20572263273924\n","powerful 113.20283492022443\n","enjoyable 117.0064118323224\n","remarkable 122.45276787104888\n","appealing 123.15363706161516\n","solid 136.937091529792\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YDy0bHbyHfIB"},"source":["#### SGD (Stochastic Gradient Descent) Classifier"]},{"cell_type":"markdown","metadata":{"id":"gIXNj0rlHfIC"},"source":["cf.\n","> [`SGDClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) The ‘log’ loss gives logistic regression model"]},{"cell_type":"code","metadata":{"id":"DgswUUy9HfIC","outputId":"52687c90-ad39-4c17-94c8-455a35e01c0e"},"source":["from sklearn.linear_model import SGDClassifier\n","log_model_sk = SGDClassifier(loss='log', penalty='none')\n","\n","log_model_sk.fit(X=X_train_sk, y=y_train_sk)\n","\n","print(\"Train acc: %.3f\" % (100 * log_model_sk.score(X_train_sk, y_train_sk)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train acc: 99.436\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HamErYZ5HfIF","outputId":"dd4f45d5-f1f2-4bf1-a7e0-298c03e222ac"},"source":["val_docs = [doc['text'] for doc in val]\n","X_dev_sk = vectorizer.transform(val_docs)\n","y_dev_sk = np.array([doc['label'] for doc in val])\n","print(\"Dev acc: %.3f\" % (100 * log_model_sk.score(X_dev_sk, y_dev_sk)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dev acc: 75.000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1xv6zJ-kHfII","outputId":"2f696d44-88a8-41af-f28c-1f5d319b9a51"},"source":["test_docs = [doc['text'] for doc in test]\n","X_test_sk = vectorizer.transform(test_docs)\n","y_test_sk = np.array([doc['label'] for doc in test])\n","print(\"Test acc: %.3f\" % (100 * log_model_sk.score(X_test_sk, y_test_sk)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test acc: 76.661\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nupW0zQ5HfIL","outputId":"117d0e50-f349-4cd2-bff3-e1d25a98ee62"},"source":["imdb_test_docs = [doc['text'] for doc in imdb_test]\n","X_imdb_test_sk = vectorizer.transform(imdb_test_docs)\n","y_imdb_test_sk = np.array([doc['label'] for doc in imdb_test])\n","print(\"IMDb Test acc: %.3f\" % (100 * log_model_sk.score(X_imdb_test_sk, y_imdb_test_sk)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["IMDb Test acc: 75.896\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hzp3uystHfIP"},"source":["---\n","### More features"]},{"cell_type":"code","metadata":{"id":"-I_UiNSNHfIQ"},"source":["def tokenize(string):\n","    ''' Bare-bones tokenization '''\n","    return [token for token in string.split()]\n","\n","def new_extract_feats(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","                         \n","    # Extract vocabulary\n","    word_counter = collections.Counter()\n","    bigram_counter = collections.Counter()\n","    for example in datasets[0]: # assume first dataset is training set\n","        tokens = tokenize(example['text'])\n","        word_counter.update(tokens)\n","        bigram_counter.update(zip(tokens, tokens[1:]))\n","    vocabulary = set([k for k, v in word_counter.most_common(10000)])\n","    bigram_vocab = set([k for k, v in bigram_counter.most_common(5000)])\n","\n","    features = set()\n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['features'] = collections.defaultdict(float)\n","            tokens = tokenize(example['text'])\n","            #Extract features (by name) for one example:\n","            word2count = collections.Counter(tokens)\n","            bigrams = collections.Counter(zip(tokens, tokens[1:]))\n","            for word, count in word2count.items():\n","                if word in vocabulary:\n","                    example[\"features\"][word] = min(1, count)\n","                #else:\n","                #    example[\"features\"][\"FEAT_UNK\"] = 1\n","                if word in [\"n't\", \"bad\", \"awful\", \"terrible\"]:\n","                    example[\"features\"][\"FEAT_negative\"] = 1\n","                if word in [\"great\", \"fantastic\", \"excellent\", \"superb\", \"awesome\"]:\n","                    example[\"features\"][\"FEAT_positive\"] = 1\n","                #example[\"features\"][\"FEAT_length\"] = len(tokenize(example['text'])) / 5\n","            for bigram in bigrams:\n","                \n","                if bigram in bigram_vocab:\n","                    example[\"features\"][\"%s_%s\" % (bigram[0], bigram[1])] = 1\n","                    \n","            features.update(example['features'].keys())\n","                            \n","    # By now, we know what all the features will be, so we can\n","    # assign indices to them.\n","    feat2idx = dict(zip(features, range(len(features))))\n","    idx2feat = {v: k for k, v in feat2idx.items()}\n","    dim = len(feat2idx)\n","                \n","    # Now we create actual vectors from those indices.\n","    for dataset in datasets:\n","        for example in dataset:\n","            example['input'] = np.zeros((dim))\n","            for feature in example['features']:\n","                example['input'][feat2idx[feature]] = example['features'][feature]\n","    return idx2feat, word_counter\n","    \n","idx2feat, word_counter = new_extract_feats([train, val, test, imdb_test]) # adds the features as a key in each example dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"uWGjSaRMHfIS","outputId":"11e22e6b-7cca-4bcd-fefc-a5c9ab7f42cf"},"source":["tmp = tokenize(train[0]['text'])\n","for pair in zip(tmp, tmp[1:]):\n","    print(pair)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('The', 'Rock')\n","('Rock', 'is')\n","('is', 'destined')\n","('destined', 'to')\n","('to', 'be')\n","('be', 'the')\n","('the', '21st')\n","('21st', 'Century')\n","('Century', \"'s\")\n","(\"'s\", 'new')\n","('new', '``')\n","('``', 'Conan')\n","('Conan', \"''\")\n","(\"''\", 'and')\n","('and', 'that')\n","('that', 'he')\n","('he', \"'s\")\n","(\"'s\", 'going')\n","('going', 'to')\n","('to', 'make')\n","('make', 'a')\n","('a', 'splash')\n","('splash', 'even')\n","('even', 'greater')\n","('greater', 'than')\n","('than', 'Arnold')\n","('Arnold', 'Schwarzenegger')\n","('Schwarzenegger', ',')\n","(',', 'Jean-Claud')\n","('Jean-Claud', 'Van')\n","('Van', 'Damme')\n","('Damme', 'or')\n","('or', 'Steven')\n","('Steven', 'Segal')\n","('Segal', '.')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f232hRYgHfIV"},"source":["model = SGDClassifier(loss='log', penalty='none')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"M2GcqaXXHfIc"},"source":["X_train = [x['input'] for x in train]\n","y_train = [y['label'] for y in train]\n","model = model.fit(X=X_train, y=y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IcyvCcxlHfIf","outputId":"34bea48c-19a1-4dc7-f2a1-842ccf6e0fc8"},"source":["X_train = [x['input'] for x in train]\n","y_train = [y['label'] for y in train]\n","train_acc = evaluate(X_train, y_train, model)\n","print(\"Train acc: %.3f\" % (100 * train_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train acc: 99.595\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1__Cy8_dHfIj","outputId":"fd456844-81ec-491c-9479-241600cdcb88"},"source":["X_dev = [x['input'] for x in val]\n","y_dev = [y['label'] for y in val]\n","dev_acc = evaluate(X_dev, y_dev, model)\n","print(\"Dev acc: %.3f\" % (100 * dev_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dev acc: 70.986\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N6XSNYAGHfIl","outputId":"e1bf78e7-bafc-426d-9151-7c3dfcf9ed6a"},"source":["X_test = [x['input'] for x in test]\n","y_test = [y['label'] for y in test]\n","test_acc = evaluate(X_test, y_test, model)\n","print(\"Test acc: %.3f\" % (100 * test_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test acc: 75.783\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zpH-ZEJ_HfIp","outputId":"deae1025-9b80-4bfe-8b4c-a11533b1e311"},"source":["X_test = [x['input'] for x in imdb_test]\n","y_test = [y['label'] for y in imdb_test]\n","test_acc = evaluate(X_test, y_test, model)\n","print(\"IMDB test acc: %.3f\" % (100 * test_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["IMDB test acc: 75.498\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"jupyter":{"outputs_hidden":true},"id":"UPrt-ByPHfIs"},"source":["---\n","## References\n","DS-GA 1012 Natural Language Understanding Spring 2019"]}]}